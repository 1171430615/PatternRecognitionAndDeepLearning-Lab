# 实验内容

* 基于 PyTorch 实现 AlexNet
* 在 Cifar-10 数据集上进行验证
* 使用tensorboard进行训练数据可视化（Loss 曲线）
* 如有条件，尝试不同参数的影响，尝试其他网络结构
* 请勿使用torchvision.models.AlexNet

# 实验环境

* Windows10
* Python 3.3
* Pytorch 1.1
* cuda 9.0

# 实验介绍

这是深度学习课程的第二个实验，实验的主要内容是卷积神经网络，要求实现 AlexNet。但是 AlexNet 的输入不符合 Cifar-10 数据集，因此这里将参数更改了一下，但网络结构没有变，还是五层卷积，三层全连接。

虽然对于 32 X 32 这么小的图片，用 AlexNet 有点大材小用的感觉，但实验要求，而且对于初学者来说，AlexNet 还是很经典的，能学到不少东西，直接干就好了。

# 实验过程

### 卷积神经网的尺寸计算

在写卷积神经网的代码之前，首先，必须要知道一个知识点，就是每层卷积神经网的输入和输出的关系，即卷积过程的尺寸变化。

尺寸的变化公式可以按照如下公式进行计算：

**N = (W − F + 2P )/S+1**

其中

> * W 表示输入图片尺寸（W×W）
> * F 表示卷积核的大小（F×F）
> * S 表示步长
> * P 表示填充的像素数（Padding）

最后输出的图像大小是 N X N

池化层的计算大致一样，也是利用滤波器的大小 F×F 和 步长的大小 S 进行计算；通常是不加 Padding 的，如果有 Padding， 则完全按照卷积的公式计算即可。

#### 1. 加载数据集

实验中使用的是 CIFAR-10 数据集，CIFAR-10 数据集由 10 个类的 60000 个 32x32 彩色图像组成，每个类有6000个图像。有50000个训练图像和10000个测试图像。 

#### 2. 定义网络结构

AlexNet 的网络结构是 5 层卷积，3 层全连接，由于输入的图片大小是 32 x 32 的，因此不能完全按照 AlexNet 设计。

网络结构见代码，每层的输出大小已经在注释中进行标注

#### 3. 训练

训练过程中，计算损失函数和准确率，最后，在所有 epoch 都跑完后，将模型保存起来。

#### 4. 测试

测试函数在第一个实验中已经写过了，这里没改什么东西


# 实验结果

3 个 epoch 的准确率大概在 42% 左右，训练后的 loss 和准确率曲线如下：

Loss 曲线：

![Loss 曲线](https://img-blog.csdnimg.cn/2019062011312387.png)

准确率曲线：
![准确率曲线](https://img-blog.csdnimg.cn/2019062011301671.png)

最后保存的模型是 20 个 epoch 后的，准确率大概在 66% 左右。

# 运行

运行代码命令：

> python .\AlexNet.py

可视化命令：

> tensorboard --logdir=./Result

